{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<td>\n",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
        "</td>"
      ],
      "metadata": {
        "id": "Ij96GtMy6pwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelspark/blob/master/notebooks/delta_table_upload_demo.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<td>\n",
        "<a href=\"https://ggithub/Labelbox/labelspark/blob/master/notebooks/delta_table_upload_demo.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ],
      "metadata": {
        "id": "XhfDur296bUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __*Labelbox Connector for Databricks Tutorial Notebook*__\n",
        "\n",
        "# _**Ingesting Data into Labelbox from Delta Tables using Labelspark**_"
      ],
      "metadata": {
        "id": "Im8cLvmGMLn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks for trying out the Databricks and Labelbox Connector! This notebook will help illustrate how Labelbox and Databricks can be used together to power unstructured data workflows.\n",
        "\n",
        "Labelbox can be used to rapidly annotate a variety of unstructured data from your Data Lake ([images](https://labelbox.com/product/image), [video](https://labelbox.com/product/video), [text](https://labelbox.com/product/text), and [geospatial tiled imagery](https://docs.labelbox.com/docs/tiled-imagery-editor)) and the Labelbox Connector for Databricks makes it easy to bring the annotations back into your Lakehouse environment for AI/ML and analytical workflows.\n",
        "\n",
        "If you would like to watch a video of the workflow, check out our [Data & AI Summit Demo](https://databricks.com/session_na21/productionizing-unstructured-data-for-ai-and-analytics).\n",
        "\n",
        "\n",
        "<img src=\"https://labelbox.com/static/images/partnerships/collab-chart.svg\" alt=\"example-workflow\" width=\"800\"/>\n",
        "\n",
        "<h5>Questions or comments? Reach out to us at ecosystem+databricks@labelbox.com"
      ],
      "metadata": {
        "id": "ditr7IItMSPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _**Documentation**_"
      ],
      "metadata": {
        "id": "KUxnxJInSu62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Rows**\n",
        "_____________________\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- A `row_data` column - This column must be URLs that point to the asset to-be-uploaded\n",
        "\n",
        "- Either a `dataset_id` column or an input argument for `dataset_id`\n",
        "  - If uploading to multiple datasets, provide a `dataset_id` column\n",
        "  - If uploading to one dataset, provide a `dataset_id` input argument\n",
        "    - _This can still be a column if it's already in your CSV file_\n",
        "\n",
        "- If uploading data from a cloud storage bucket, make sure you set up the proper [Labelbox integration](https://docs.labelbox.com/docs/iam-delegated-access) for that cloud storage service\n",
        "\n",
        "**Recommended:**\n",
        "- A `global_key` column\n",
        "  - This column contains unique identifiers for your data rows\n",
        "  - If none is provided, will default to your `row_data` column\n",
        "  - The Labelspark client will validate the uniqueness of the global keys provided in this column. If one or more of the global keys already exist in your organization, the client will use the `skip_duplicates` argument of `create_data_rows_from_delta_table()` to determine how to proceed.\n",
        "\n",
        "    - `skip_duplicates = True`: Rows with existing global keys will not be uploaded\n",
        "    - `skip_duplicates = False`: Rows with existing global keys will have a suffix appended to the global key to ensure uniqueness.\n",
        "\n",
        "\n",
        "**Optional:**\n",
        "- A `project_id` columm or an input argument for `project_id`\n",
        "  - If batching to multiple projects, provide a `project_id` column\n",
        "  - If batching to one project, provide a `project_id` input argument\n",
        "    - _This can still be a column if it's already in your Delta Table_"
      ],
      "metadata": {
        "id": "d-PBix9pSv2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _**Code**_"
      ],
      "metadata": {
        "id": "nYRH8tfHTJEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages"
      ],
      "metadata": {
        "id": "HKqi5AcxTKSw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w24tOwvi5JNd"
      },
      "outputs": [],
      "source": [
        "%pip install labelspark  -q\n",
        "%pip install delta-spark -q\n",
        "%pip install pyspark -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate Clients and create a new Labelbox dataset. This is the dataset that we will use for ingesting data from a Delta table into Labelbox.\n",
        "\n",
        "We will also be creating a new project and attaching the uploaded data for labeling."
      ],
      "metadata": {
        "id": "Lyy8pE_-WquW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import labelspark as ls\n",
        "import labelbox as lb\n",
        "\n",
        "API_KEY = \"\"\n",
        "client = ls.Client(API_KEY)\n",
        "\n",
        "lb_client = lb.Client(API_KEY)\n",
        "dataset = lb_client.create_dataset(name=\"Databricks Integration Demo Dataset\", iam_integration=None)\n",
        "project = lb_client.create_project(name=\"Databricks Integration Demo Project\", media_type=lb.MediaType.Image)"
      ],
      "metadata": {
        "id": "X4qyusrlW38b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required dependencies if you are uploading data from a Delta table stored in a third-party cloud service.\n",
        "\n",
        "AWS S3 dependencies:"
      ],
      "metadata": {
        "id": "RMXEYd4FTQeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /usr/local/lib/python3.10/dist-packages/pyspark/jars https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar\n",
        "!wget -P /usr/local/lib/python3.10/dist-packages/pyspark/jars https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar"
      ],
      "metadata": {
        "id": "4-ICLmvn5mSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GCS dependencies:"
      ],
      "metadata": {
        "id": "fncOoUfm0g1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /usr/local/lib/python3.10/dist-packages/pyspark/jars https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar"
      ],
      "metadata": {
        "id": "gVFX2uY69u4n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spark_config argument is only required when connecting to a third-party storage service such as AWS S3 or GCS, the fields that need to be in spark_config are:\n",
        "\n",
        "\n",
        "*   jars - A string containing the path to the .jar files needed for the connector you are using, separated by a comma. The .jar file versions will need to be compatible with the version of Spark you have installed.\n",
        "\n",
        "  *   AWS S3 - hadoop-aws-2.7.4.jar, aws-java-sdk-1.7.4.jar\n",
        "  *   GCS - gcs-connector-hadoop3-latest.jar\n",
        "*   credentials - Only required if connecting to GCS, the value should be the path to your [GCS service account](https://developers.google.com/workspace/guides/create-credentials#create_credentials_for_a_service_account) credentials JSON file. To run this notebook with a GCS credentials file, click on the folder icon on the left sidebar and then click on the upload file icon. This will upload the file to the Notebook's `/content/` folder.\n",
        "\n",
        "\n",
        "*   AWS_ACCCESS_KEY - Only required if connecting to AWS S3, the value should be your [AWS service account](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) access key\n",
        "*   AWS_SECRET_KEY - Only required if connecting to AWS S3, the value should be your [AWS service account](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) secret key\n",
        "\n"
      ],
      "metadata": {
        "id": "y8b9Awq7YAfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "SPARK_HOME = '/usr/local/lib/python3.10/dist-packages/pyspark/'\n",
        "\n",
        "AWS_ACCESS_KEY = ''\n",
        "AWS_SECRET_KEY = ''\n",
        "\n",
        "aws_spark_config = {\n",
        "                \"jars\": f\"{SPARK_HOME}/jars/hadoop-aws-2.7.4.jar, {SPARK_HOME}/jars/aws-java-sdk-1.7.4.jar\",\n",
        "                \"AWS_ACCESS_KEY\": AWS_ACCESS_KEY,\n",
        "                \"AWS_SECRET_KEY\": AWS_SECRET_KEY\n",
        "}\n",
        "\n",
        "gcs_credentials_file = '/content/{name of credentials .json file}'\n",
        "\n",
        "gcs_spark_config = {\n",
        "                \"jars\": f\"{SPARK_HOME}/jars/gcs-connector-hadoop3-latest.jar\",\n",
        "                \"credentials\": gcs_credentials_file\n",
        "}\n",
        "\n",
        "#table_path can either be a local file, or a link to a third-party cloud storage service (e.g gs://{bucket_name}/{folder}/{table} or s3a://{bucket_name}/{folder}/{table})\n",
        "table_path = \"\""
      ],
      "metadata": {
        "id": "hlnrhqhoEKPc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't have a Delta Table ready for this demo, run the following cell to create one and save it to this notebook's local storage:"
      ],
      "metadata": {
        "id": "1O4d4SWCiH84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{'row_data': 'https://lb-test-data.s3.us-west-1.amazonaws.com/image-samples/sample-image-1.jpg', 'global_key': 'sample-image-1.jpg'}]\n",
        "table_path = '/content/demo-table'\n",
        "\n",
        "builder = SparkSession.builder.appName(\"labelspark_export\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.write.format('delta').save(table_path)\n"
      ],
      "metadata": {
        "id": "tfdKl-5ZiRqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make sure the Delta table saved properly\n",
        "df = spark.read.load(table_path)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iXgAkqNkGRP",
        "outputId": "2d9354dd-fa09-439a-9e8e-0ce406d82a24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------------------+\n",
            "|        global_key|            row_data|\n",
            "+------------------+--------------------+\n",
            "|sample-image-1.jpg|https://lb-test-d...|\n",
            "+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#When uploading data from a Delta table saved locally, only the dataset_id and table_path argument are required. The project_id argument\n",
        "#is only necessary if you want the uploaded data rows to be batched to a project\n",
        "results = client.create_data_rows_from_delta_table(dataset_id=dataset.uid, project_id=project.uid, verbose=True, table_path=table_path)"
      ],
      "metadata": {
        "id": "K0gpeUq2kf9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can either create a Spark Session before running `create_data_rows_from_delta_table()`, or we can pass in the `spark_config` argument that the Labelspark Client will use to programatically create the Spark Session."
      ],
      "metadata": {
        "id": "nZQfLzZ8XINp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS:\n",
        "\n",
        "Build Spark Session before running `create_data_rows_from_delta_table()`:"
      ],
      "metadata": {
        "id": "61q1CNPH1vxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder = SparkSession.\\\n",
        "        builder.\\\n",
        "        appName(\"pyspark-test\").\\\n",
        "        config(\"spark.driver.extraClassPath\", f\"{SPARK_HOME}/jars/hadoop-aws-2.7.4.jar:{SPARK_HOME}/jars/aws-java-sdk-1.7.4.jar\").\\\n",
        "        config(\"spark.executor.extraClassPath\", f\"{SPARK_HOME}/jars/hadoop-aws-2.7.4.jar:{SPARK_HOME}/jars/aws-java-sdk-1.7.4.jar\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
        "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
        "\n",
        "results = client.create_data_rows_from_delta_table(dataset_id=dataset.uid, project_id=project.uid, verbose=True, table_path=table_path, spark=spark)"
      ],
      "metadata": {
        "id": "RI13jipJ5-vO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run `create_data_rows_from_delta_table()` with spark_config arg:"
      ],
      "metadata": {
        "id": "5AH3hlr96zLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = client.create_data_rows_from_delta_table(dataset_id=dataset.uid, project_id=project.uid, verbose=True, table_path=table_path, spark_config=aws_spark_config)"
      ],
      "metadata": {
        "id": "Vpsssuy303g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the status of the upload results to make sure the upload was successful\n",
        "results"
      ],
      "metadata": {
        "id": "50Wml9n1oKX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCS:\n",
        "\n",
        "\n",
        "Build Spark Session before running `create_data_rows_from_delta_table()`:"
      ],
      "metadata": {
        "id": "aDSYKdZ91xUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder = SparkSession.\\\n",
        "        builder.\\\n",
        "        appName(\"pyspark-demo\").\\\n",
        "        config(\"spark.driver.extraClassPath\", f\"{SPARK_HOME}/jars/gcs-connector-hadoop3-latest.jar\").\\\n",
        "        config(\"spark.executor.extraClassPath\", f\"{SPARK_HOME}/jars/gcs-connector-hadoop3-latest.jar\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "spark.conf.set(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\n",
        "spark.conf.set(\"google.cloud.auth.type\", \"SERVICE_ACCOUNT_JSON_KEYFILE\")\n",
        "spark.conf.set('google.cloud.auth.service.account.json.keyfile', credentials_file)\n",
        "spark.conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
        "\n",
        "results = client.create_data_rows_from_delta_table(dataset_id=dataset.uid, verbose=True, table_path=table_path, spark=spark)"
      ],
      "metadata": {
        "id": "oAx96Yoj9Egg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run `create_data_rows_from_delta_table()` with spark_config arg:"
      ],
      "metadata": {
        "id": "scLTXsoc8dRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = client.create_data_rows_from_delta_table(dataset_id=dataset.uid, verbose=True, table_path=table_path, spark_config=gcs_spark_config)"
      ],
      "metadata": {
        "id": "lIxKmJax15HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the status of the upload results to make sure the upload was successful\n",
        "results"
      ],
      "metadata": {
        "id": "OxSCV_AblvBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}