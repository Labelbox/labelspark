{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Labelbox <> Databricks Pipeline Creator</font></h1>\n",
    "<b>This script sets up a job to upload data to Labelbox, a data labeling platform. It includes information like where the data is coming from, what specific tasks need to be done, and how often the job should run.\n",
    "\n",
    "If the script is set to \"continuous,\" the job will keep running without breaks. Otherwise, it will follow a specific schedule.\n",
    "\n",
    "Once everything is set up, the script sends a request to Databricks, a data analytics platform, to create the job. If everything goes as planned, it will print a success message; otherwise, it will print an error.</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Confgure the code using the cell below. </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- CONFIGURATION -----\n",
    "\n",
    "# User-defined variables\n",
    "# Databricks cloud instance URL. make sure the URL is in the format <workspace_id>.<cloud>.databricks.com\n",
    "databricks_instance = \"\"\n",
    "# Personal access token for Databricks authentication. This can be generated from the user settings page.\n",
    "databricks_api__key = \"\"\n",
    "# Path to the table which needs to be processed. For example \"<metastore>.<database>.<table>\"\n",
    "table_path = \"\"\n",
    "# API Key for Labelbox integration. This can be generated from the Labelbox settings page.\n",
    "labelbox_api__key = \"\"\n",
    "# ID of the dataset to be used. \n",
    "dataset_id = \"\"\n",
    "# Frequency of running the job. Can be set to 'continuous' or a specific cron schedule.\n",
    "frequency = \"\"  # Examples: \"continuous\" or \"0 0/5 * * * ?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema_map = [\n",
    "    (\"row_data\", \"row_data\"),\n",
    "    (\"id\", \"global_key\")\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Do not edit the code below this line unless you want to enable addtional/custom functionality</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# ----- JOB SCHEDULING LOGIC -----\n",
    "\n",
    "# If the job needs to run continuously, use the \"continuous\" block\n",
    "# Else, use the \"schedule\" block with the specified cron frequency\n",
    "if frequency == \"continuous\":\n",
    "    schedule_block = {\n",
    "        \"continuous\": {\n",
    "            \"pause_status\": \"UNPAUSED\"\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    schedule_block = {\n",
    "        \"schedule\": {\n",
    "            \"quartz_cron_expression\": frequency,\n",
    "            \"timezone_id\": \"UTC\",\n",
    "            \"pause_status\": \"UNPAUSED\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ----- JOB DEFINITION -----\n",
    "\n",
    "# Define the parameters and structure of the job to be created in Databricks\n",
    "payload = {\n",
    "    \"run_as\": {\"user_name\": email},\n",
    "    \"name\": \"upload_to_labelbox\",\n",
    "    \"email_notifications\": {\"no_alert_for_skipped_runs\": False},\n",
    "    \"webhook_notifications\": {},\n",
    "    \"timeout_seconds\": 0,\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"upload_to_labelbox\",\n",
    "            \"run_if\": \"ALL_SUCCESS\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"notebooks/databricks_pipeline_creator/upload_to_labelbox\",\n",
    "                \"base_parameters\": {\n",
    "                    \"dataset_id\": dataset_id,\n",
    "                    \"table_path\": table_path,\n",
    "                    \"labelbox_api_key\": labelbox_api_key,\n",
    "                },\n",
    "                \"source\": \"GIT\"\n",
    "            },\n",
    "            \"job_cluster_key\": \"Job_cluster\",\n",
    "            \"libraries\": [\n",
    "                {\"pypi\": {\"package\": \"labelspark\"}},\n",
    "                {\"pypi\": {\"package\": \"labelbox==3.49.1\"}},\n",
    "                {\"pypi\": {\"package\": \"numpy==1.25\"}},\n",
    "                {\"pypi\": {\"package\": \"opencv-python==4.8.0.74\"}}\n",
    "            ],\n",
    "            \"timeout_seconds\": 0,\n",
    "            \"email_notifications\": {},\n",
    "            \"notification_settings\": {\n",
    "                \"no_alert_for_skipped_runs\": False,\n",
    "                \"no_alert_for_canceled_runs\": False,\n",
    "                \"alert_on_last_attempt\": False\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"job_clusters\": [\n",
    "        {\n",
    "            \"job_cluster_key\": \"Job_cluster\",\n",
    "            \"new_cluster\": {\n",
    "                \"cluster_name\": \"\",\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"gcp_attributes\": {\n",
    "                    \"use_preemptible_executors\": False,\n",
    "                    \"availability\": \"ON_DEMAND_GCP\",\n",
    "                    \"zone_id\": \"HA\"\n",
    "                },\n",
    "                \"node_type_id\": \"n2-highmem-4\",\n",
    "                \"enable_elastic_disk\": True,\n",
    "                \"data_security_mode\": \"SINGLE_USER\",\n",
    "                \"runtime_engine\": \"STANDARD\",\n",
    "                \"autoscale\": {\n",
    "                    \"min_workers\": 1,\n",
    "                    \"max_workers\": 10\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"git_source\": {\n",
    "        \"git_url\": \"https://github.com/Labelbox/labelspark.git\",\n",
    "        \"git_provider\": \"gitHub\",\n",
    "        \"git_branch\": \"master\"\n",
    "    },\n",
    "    \"format\": \"MULTI_TASK\"\n",
    "}\n",
    "\n",
    "# Merge the scheduling configuration into the main job payload\n",
    "payload.update(schedule_block)\n",
    "\n",
    "# ----- JOB CREATION -----\n",
    "\n",
    "# Formulate the endpoint URL for the Databricks REST API job creation\n",
    "url = f\"https://{databricks_instance}/api/2.0/jobs/create\"\n",
    "# Define the authentication headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {databricks_api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Send the POST request to Databricks to create the job\n",
    "response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "# ----- RESPONSE HANDLING -----\n",
    "\n",
    "# Print the response\n",
    "# If the response code is 200, it means the job was created successfully.\n",
    "# Otherwise, print the error message received.\n",
    "if response.status_code == 200:\n",
    "    print(\"Job created successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to create job. Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
