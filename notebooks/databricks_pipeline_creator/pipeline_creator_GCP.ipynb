{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Labelbox <> Databricks Connector\n",
    "\n",
    "This script guides through the set up the Labelbox <> Databricks connector. Once set up, it will ingest data from Databricks to Labelbox.</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets --quiet\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: User inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is ingested from a Databricks table: enter your Databricks info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "# Databricks instance URL\n",
    "databricks_instance_label = widgets.Label('Enter your Databricks cloud instance URL:')\n",
    "databricks_instance = widgets.Text(value=' <workspace_id>.<cloud>.databricks.com')\n",
    "display(databricks_instance_label, databricks_instance)\n",
    "\n",
    "# Personal access token for Databricks authentication\n",
    "databricks_api_key_label = widgets.Label('Enter your Databricks personal access token:')\n",
    "databricks_api_key = widgets.Password(value='')\n",
    "display(databricks_api_key_label, databricks_api_key)\n",
    "\n",
    "# Databricks table path\n",
    "table_path_label = widgets.Label('Enter the path to the Databricks table to be ingested:')\n",
    "table_path = widgets.Text(value='<metastore>.<database>.<table>')\n",
    "display(table_path_label, table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is connected to a Labelbox dataset: enter your Labelbox info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "# Labelbox API Key\n",
    "labelbox_api_key_label = widgets.Label('Labelbox API Key. This can be generated from the Labelbox settings page:')\n",
    "labelbox_api_key = widgets.Password(value='')\n",
    "display(labelbox_api_key_label, labelbox_api_key)\n",
    "\n",
    "\n",
    "# Create a new dataset in Labelbox?\n",
    "create_new_dataset_label = widgets.Label('Do you wish to create a new dataset in Labelbox for this data ingestion process?')\n",
    "create_new_dataset = widgets.Dropdown(options=[\"yes\", \"no\"], value=\"no\")\n",
    "\n",
    "# Dataset name to create in Labelbox\n",
    "dataset_name_label = widgets.Label('Enter the name of the dataset to create in Labelbox for this data ingestion process:')\n",
    "dataset_name = widgets.Text(value='')\n",
    "\n",
    "# Labelbox dataset ID\n",
    "dataset_id_label = widgets.Label('Enter the Labelbox dataset ID to use for this data ingestion process:')\n",
    "dataset_id = widgets.Text(value='')\n",
    "\n",
    "# Box for dataset name\n",
    "dataset_name_box = widgets.VBox([dataset_name_label, dataset_name])\n",
    "# Box for dataset id\n",
    "dataset_id_box = widgets.VBox([dataset_id_label, dataset_id])\n",
    "\n",
    "# Output box to conditionally display widgets\n",
    "output_box = widgets.Output()\n",
    "\n",
    "def on_change(change):\n",
    "    with output_box:\n",
    "        if change['new'] == \"yes\":\n",
    "            display(dataset_name_box)\n",
    "        else:\n",
    "            display(dataset_id_box)\n",
    "        output_box.clear_output(wait=True)  # Clear the previous widgets\n",
    "\n",
    "# Attach the function as observer to the dropdown's value\n",
    "create_new_dataset.observe(on_change, names='value')\n",
    "\n",
    "display(create_new_dataset_label, create_new_dataset, output_box)\n",
    "\n",
    "# Trigger the observer to display the correct box initially\n",
    "on_change({'new': create_new_dataset.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the parameters of the data ingestion connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "# Schedule type input\n",
    "schedule_type_label = widgets.Label('Do you wish to trigger the data ingestion: manually from Labelbox and/or Databricks or, on a set interval (eg weekly)?')\n",
    "schedule_type = widgets.Dropdown(options=[\"manual\", \"interval\"], value=\"interval\")\n",
    "\n",
    "# Frequency input\n",
    "frequency_label = widgets.Label('If you have selected \"interval\", enter the interval to use to trigger the data ingestion pipeline:')\n",
    "frequency = widgets.Dropdown(options=[\"daily\", \"weekly\"], value=\"weekly\", description='Frequency:')\n",
    "\n",
    "# Start and end date inputs\n",
    "start_ingestion = widgets.DatePicker(value=None, description='Start Date:')\n",
    "end_ingestion = widgets.DatePicker(value=None, description='End Date:')\n",
    "\n",
    "# Box for interval details\n",
    "interval_box = widgets.VBox([frequency_label, frequency, start_ingestion, end_ingestion])\n",
    "empty_box = widgets.VBox([])\n",
    "\n",
    "# Output box to conditionally display widgets\n",
    "output_box = widgets.Output()\n",
    "\n",
    "def on_schedule_type_change(change):\n",
    "    with output_box:\n",
    "        output_box.clear_output(wait=True)  # Clear previous widgets\n",
    "        if change['new'] == \"interval\":\n",
    "            display(interval_box)\n",
    "        else:\n",
    "            display(empty_box)\n",
    "        output_box.clear_output(wait=True)  # Clear the previous widgets\n",
    "\n",
    "# Attach the function as observer to the dropdown's value\n",
    "schedule_type.observe(on_schedule_type_change, names='value')\n",
    "\n",
    "display(schedule_type_label, schedule_type, output_box)\n",
    "\n",
    "# Trigger the observer to display the correct box initially\n",
    "on_schedule_type_change({'new': schedule_type.value})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dry run of the data ingestion pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that that it worked: links links links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up the data ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# ----- JOB SCHEDULING LOGIC -----\n",
    "\n",
    "# If the job needs to run continuously, use the \"continuous\" block\n",
    "# Else, use the \"schedule\" block with the specified cron frequency\n",
    "if frequency == \"continuous\":\n",
    "    schedule_block = {\n",
    "        \"continuous\": {\n",
    "            \"pause_status\": \"UNPAUSED\"\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    schedule_block = {\n",
    "        \"schedule\": {\n",
    "            \"quartz_cron_expression\": frequency,\n",
    "            \"timezone_id\": \"UTC\",\n",
    "            \"pause_status\": \"UNPAUSED\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ----- JOB DEFINITION -----\n",
    "\n",
    "# Define the parameters and structure of the job to be created in Databricks\n",
    "payload = {\n",
    "    \"run_as\": {\"user_name\": email},\n",
    "    \"name\": \"upload_to_labelbox\",\n",
    "    \"email_notifications\": {\"no_alert_for_skipped_runs\": False},\n",
    "    \"webhook_notifications\": {},\n",
    "    \"timeout_seconds\": 0,\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"upload_to_labelbox\",\n",
    "            \"run_if\": \"ALL_SUCCESS\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"notebooks/databricks_pipeline_creator/upload_to_labelbox\",\n",
    "                \"base_parameters\": {\n",
    "                    \"dataset_id\": dataset_id,\n",
    "                    \"table_path\": table_path,\n",
    "                    \"labelbox_api_key\": labelbox_api_key,\n",
    "                },\n",
    "                \"source\": \"GIT\"\n",
    "            },\n",
    "            \"job_cluster_key\": \"Job_cluster\",\n",
    "            \"libraries\": [\n",
    "                {\"pypi\": {\"package\": \"labelspark\"}},\n",
    "                {\"pypi\": {\"package\": \"labelbox==3.49.1\"}},\n",
    "                {\"pypi\": {\"package\": \"numpy==1.25\"}},\n",
    "                {\"pypi\": {\"package\": \"opencv-python==4.8.0.74\"}}\n",
    "            ],\n",
    "            \"timeout_seconds\": 0,\n",
    "            \"email_notifications\": {},\n",
    "            \"notification_settings\": {\n",
    "                \"no_alert_for_skipped_runs\": False,\n",
    "                \"no_alert_for_canceled_runs\": False,\n",
    "                \"alert_on_last_attempt\": False\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"job_clusters\": [\n",
    "        {\n",
    "            \"job_cluster_key\": \"Job_cluster\",\n",
    "            \"new_cluster\": {\n",
    "                \"cluster_name\": \"\",\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"gcp_attributes\": {\n",
    "                    \"use_preemptible_executors\": False,\n",
    "                    \"availability\": \"ON_DEMAND_GCP\",\n",
    "                    \"zone_id\": \"HA\"\n",
    "                },\n",
    "                \"node_type_id\": \"n2-highmem-4\",\n",
    "                \"enable_elastic_disk\": True,\n",
    "                \"data_security_mode\": \"SINGLE_USER\",\n",
    "                \"runtime_engine\": \"STANDARD\",\n",
    "                \"autoscale\": {\n",
    "                    \"min_workers\": 1,\n",
    "                    \"max_workers\": 10\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"git_source\": {\n",
    "        \"git_url\": \"https://github.com/Labelbox/labelspark.git\",\n",
    "        \"git_provider\": \"gitHub\",\n",
    "        \"git_branch\": \"master\"\n",
    "    },\n",
    "    \"format\": \"MULTI_TASK\"\n",
    "}\n",
    "\n",
    "# Merge the scheduling configuration into the main job payload\n",
    "payload.update(schedule_block)\n",
    "\n",
    "# ----- JOB CREATION -----\n",
    "\n",
    "# Formulate the endpoint URL for the Databricks REST API job creation\n",
    "url = f\"https://{databricks_instance}/api/2.0/jobs/create\"\n",
    "# Define the authentication headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {databricks_api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Send the POST request to Databricks to create the job\n",
    "response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "# ----- RESPONSE HANDLING -----\n",
    "\n",
    "# Print the response\n",
    "# If the response code is 200, it means the job was created successfully.\n",
    "# Otherwise, print the error message received.\n",
    "if response.status_code == 200:\n",
    "    print(\"Job created successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to create job. Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that that it worked: links links links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Monitoring and troubleshooting the data ingestion pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print all the links for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Link to Databricks instance: {'xxxx'}\")\n",
    "print(f\"Link to Databricks table: {'xxxx'}\")\n",
    "print(f\"Link to view ingestion job: {'xxxx'}\")\n",
    "print(f\"Link to failed ingestion job run: {'xxxx'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
