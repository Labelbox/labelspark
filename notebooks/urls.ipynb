{"cells":[{"cell_type":"markdown","source":["<td>\n   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n</td>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bd1df061-02be-4232-8016-4aeb27fd7691","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# __*Labelbox Connector for Databricks Tutorial Notebook*__\n\n# _**Creating Data Rows with LabelSpark**_"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"14187edd-08f4-4002-a90a-012c1c8857cc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<td>\n<a href=\"https://github.com/Labelbox/labelspark/blob/master/urls.ipynb\" target=\"_blank\"><img\nsrc=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n</td>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b2701875-c3d4-41cf-a964-a03129dbdd1d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Thanks for trying out the Databricks and Labelbox Connector! You or someone from your organization signed up for a Labelbox trial through Databricks Partner Connect. This notebook was loaded into your Shared directory to help illustrate how Labelbox and Databricks can be used together to power unstructured data workflows. \n\nLabelbox can be used to rapidly annotate a variety of unstructured data from your Data Lake ([images](https://labelbox.com/product/image), [video](https://labelbox.com/product/video), [text](https://labelbox.com/product/text), and [geospatial tiled imagery](https://docs.labelbox.com/docs/tiled-imagery-editor)) and the Labelbox Connector for Databricks makes it easy to bring the annotations back into your Lakehouse environment for AI/ML and analytical workflows. \n\nIf you would like to watch a video of the workflow, check out our [Data & AI Summit Demo](https://databricks.com/session_na21/productionizing-unstructured-data-for-ai-and-analytics). \n\n\n<img src=\"https://labelbox.com/static/images/partnerships/collab-chart.svg\" alt=\"example-workflow\" width=\"800\"/>\n\n<h5>Questions or comments? Reach out to us at [ecosystem+databricks@labelbox.com](mailto:ecosystem+databricks@labelbox.com)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"889e8c34-5b3d-4063-bbc9-0099251817ca","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## _**Documentation**_"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c02cc95a-899a-438f-9494-73b5befa4498","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### **Data Rows**\n_____________________\n\n**Requirements:**\n\n- A `row_data` column - This column must be URLs that point to the asset to-be-uploaded\n\n- Either a `dataset_id` column or an input argument for `dataset_id`\n  - If uploading to multiple datasets, provide a `dataset_id` column \n  - If uploading to one dataset, provide a `dataset_id` input argument\n    - _This can still be a column if it's already in your CSV file_\n\n**Recommended:**\n- A `global_key` column\n  - This column contains unique identifiers for your data rows\n  - If none is provided, will default to your `row_data` column\n- An `external_id` column\n  - This column contains non-unique identifiers for your data rows\n  - If none is provided, will default to your `global_key` column  \n\n**Optional:**\n- A `project_id` columm or an input argument for `project_id`\n  - If batching to multiple projects, provide a `project_id` column\n  - If batching to one project, provide a `project_id` input argument\n    - _This can still be a column if it's already in your CSV file_"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"eccb2677-9d24-471e-b525-56ecb426949e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## _**Code**_"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"06ff0dac-d4ab-4182-bd6a-735781250779","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Install LabelSpark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d4f269c1-f02b-47e9-9e92-711493d26d24","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%pip install labelspark -q"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6c57f9d9-4886-4a84-90f9-1985b238211f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import labelspark as ls"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0bc40908-37a9-4723-97cb-6a7d5fce7ab4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["table_name = \"labelspark_demo\" # Name to register test table under\ncsv_path = \"https://raw.githubusercontent.com/Labelbox/labelspark/master/datasets/urls.csv\" # Path to your CSV file\napi_key = \"\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"acdfe8c6-1d76-46af-a945-162f5c8a1e26","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Load a CSV as a Spark Table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a9d30902-fce5-494a-99d2-8b6c4780a8c9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('labelspark-demo').getOrCreate() # Create a Spark Session\ndf = pd.read_csv(csv_path) # Read CSV in as Pandas DataFrame\ntable = spark.createDataFrame(df) # Convert your Pandas DataFrame into a Spark DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cc3a398f-cb16-48d8-81c5-8fd2da9ff833","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Register your Spark Table (for this demo, we will only save as a temporary table)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e4811378-84d6-4e3b-9553-c98b98578a26","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["tblList = spark.catalog.listTables()\n\nif not any([x.name == table_name for x in tblList]):\n  table.createOrReplaceTempView(table_name)\n  print(f\"Registered table: {table_name}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bfaaf86c-3497-4096-a04e-95fa1bb3a576","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(sqlContext.sql(f\"select * from {table_name} LIMIT 5\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"24a15063-ee1b-49b4-8ee7-40f8f2ac8b46","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Create a project, dataset and ontology"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b186a2ad-0640-435e-88e8-a94b7439b3c3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["client = ls.Client(lb_api_key=api_key)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"548f5907-af40-49cd-bb5e-80385a2d5e98","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dataset = client.lb_client.create_dataset(name=\"LabelSpark-demo\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d5eaac1a-1284-4785-b7e3-4ef64f09a267","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Upload to Labelbox"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e711f5c6-c99f-449d-83ef-4de53b5c11d4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["results = client.create_data_rows_from_table(\n    table = table,\n    dataset_id = dataset.uid,\n    skip_duplicates = False, # If True, will skip data rows where a global key is already in use\n    verbose = True, # If True, prints information about code execution\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c81d65c8-b519-4331-9ac2-d4d8f23efc73","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"urls","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1220397246288670}},"nbformat":4,"nbformat_minor":0}
